{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/morganmcg1/control-ui/blob/main/Copy_of_Fine_tune_Donut_on_UI_RefExp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNMqJ821yNVo"
      },
      "source": [
        "# Fine-tune Donut 🍩 on UI RefExp\n",
        "\n",
        "> _NOTE_: This notebook is based on the [Donut fine-tuning notebooks by Niels Rogge](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/Donut). \n",
        "\n",
        "In this notebook, we'll fine-tune [Donut](https://huggingface.co/docs/transformers/model_doc/donut) (which is an instance of [`VisionEncoderDecoderModel`](https://huggingface.co/docs/transformers/model_doc/vision-encoder-decoder)) on a [UI RefExp dataset](https://huggingface.co/datasets/ivelin/ui_refexp_saved), which is a dataset consisting of triplets: (UI screenshot, prompt, and target bounding box). This way, the model will learn to look at a screenshot image, and answer a prompt referring to a UI component. For example: \"select the search icon next to the menu drawer\". This could be useful for tasks such as converting natural language app documentation to exectuable tests, bug reporting front end test automation and app support chat bots.\n",
        "\n",
        "Multiple specialized models have been proposed to solve the UI RefExp task in recent years: [seq2act](https://paperswithcode.com/paper/mapping-natural-language-instructions-to), [UIBert](https://paperswithcode.com/paper/uibert-learning-generic-multimodal), [pix2struct](https://paperswithcode.com/paper/pix2struct-screenshot-parsing-as-pretraining) and others.\n",
        "\n",
        "Here we will use Donut - an OCR-free Document Understanding model with state of the art performance as of 2022. Donut showed SOTA performance across several natural languages and document understanding tasks. Since UI components use language labels as well as image icons such Material Design and emoticons, we hope that it's ability to extract features from multiple languages will transfer to non-textual UI component features.\n",
        "\n",
        "Let's find out if Donut can be fine tuned to perform well on the UI RefExp task even though it was not originally designed for it. The intuition is that paper documents have similar visual and language comprehension challenges as UI screens. Since Donut is OCR-free multi-modal model, it should be able to pick up visual and text features in UI components as well as spacial relationships between them.\n",
        "\n",
        "In [Grounding Natural Language Instructions: Can Large Language Models Capture Spatial Information?](https://arxiv.org/abs/2109.08634) it was shown that Document Understanding models such as LayoutLM are able to outperform previous models specialized on UI tasks. However the paper focuses on UI components with text labels. Ideally we would like our model to be able to also understand non-textual components.\n",
        "\n",
        "We will start from the DocVQA pre-trained Donut model and repurpose the DocVQA fine tuning notebook so that the model learns to output bounding box coordinates of the referred component instead of its text label.\n",
        "\n",
        "Let's get started!\n",
        "\n",
        "## Set-up environment\n",
        "\n",
        "Credit: notebook modified from: https://github.com/ivelin/donut_ui_refexp/tree/main,
        "\n",
        "First, let's install the relevant libraries:\n",
        "* 🤗 Transformers, for the model\n",
        "* 🤗 Datasets, for loading + processing the data\n",
        "* PyTorch Lightning, for training the model\n",
        "* Weights and Biases, for logging metrics during training\n",
        "* Sentencepiece, used for tokenization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ot1nP9YHz8co"
      },
      "outputs": [],
      "source": [
        "#@title Let's install required dependencies\n",
        "\n",
        "# !pip install -q git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q git+https://github.com/morganmcg1/transformers.git@fix_dec "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip uninstall -y transformers"
      ],
      "metadata": {
        "id": "WzHSv0JnY9Sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/huggingface/transformers.git\n",
        "# !cd transformers && pip install -e ."
      ],
      "metadata": {
        "id": "zpjEYiORYz3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqcGNPJHyOlt"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMZ6tiMB1JxD"
      },
      "outputs": [],
      "source": [
        "!pip install -q pytorch-lightning wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Let's see what system resources we are running on\n",
        "import os\n",
        "print(f\"CPU Count: {os.cpu_count()}\")"
      ],
      "metadata": {
        "id": "1kf0n5wkudMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "714_cbKavUrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def cpu():\n",
        "    return torch.device('cpu')\n",
        "def gpu(i=0):\n",
        "    return torch.device(f'cuda:{i}')\n",
        "cpu(), gpu(), gpu(1)"
      ],
      "metadata": {
        "id": "O7ui-jfWx4TZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def num_gpus():\n",
        "    return torch.cuda.device_count()\n",
        "num_gpus()"
      ],
      "metadata": {
        "id": "VQ1tIEgDyCJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "TvERoforv2qS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Login to HuggingFace hub so we can save our trained model checkpoints\n",
        "# !huggingface-cli login"
      ],
      "metadata": {
        "id": "PdgBa6qK3nup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Login to Weights&Biases so we can log and chart our training metrics\n",
        "!wandb login"
      ],
      "metadata": {
        "id": "GWoh-cGbnWpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWYic8VNyDNU"
      },
      "source": [
        "## Load dataset\n",
        "\n",
        "Next, let's load the dataset from the [hub](https://huggingface.co/datasets/ivelin/rico_refexp_combined). We're prepared a RICO based dataset with combined synthetic and crowdsourced UI referring expressions. The first 15K samples in the dataset are crowdsourced. The rest of the 350K samples are synthetically generated.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hU27XC2yEot"
      },
      "outputs": [],
      "source": [
        "REFEXP_DATASET_NAME = \"ivelin/rico_refexp_combined\"\n",
        "\n",
        "# Pick which pretrained checkpoint to start the fine tuning process from\n",
        "REFEXP_MODEL_CHECKPOINT = 'ivelin/donut-refexp-combined-v1'\n",
        "\n",
        "# SMALLER\n",
        "REFEXP_DATASET_NAME = \"ivelin/ui_refexp_saved\"\n",
        "\n",
        "REFEXP_MODEL_CP_BRANCH = 'main' \n",
        "# revision: '348ddad8e958d370b7e341acd6050330faa0500f' # Iou = 0.47\n",
        "# revision: '41210d7c42a22e77711711ec45508a6b63ec380f'# : IoU=0.42 # \n",
        "# use 'main' for latest revision\n",
        "\n",
        "# REFEXP_MODEL_CHECKPOINT = \"ivelin/donut-refexp-draft\"\n",
        "# REFEXP_MODEL_CHECKPOINT = \"naver-clova-ix/donut-base\"\n",
        "# REFEXP_MODEL_CHECKPOINT = \"ivelin/donut-docvqa-demo\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(REFEXP_DATASET_NAME, num_proc=8)"
      ],
      "metadata": {
        "id": "hSguzMVA-KCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen, the dataset contains a training, a validation and a test split. And each example consists of an image, a prompt, and a target bounding box."
      ],
      "metadata": {
        "id": "wjI5uyk48V-g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DYk7tDBy-ys"
      },
      "outputs": [],
      "source": [
        "print(dataset['train'].info)\n",
        "print(dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset[\"train\"])"
      ],
      "metadata": {
        "id": "js8hkYGqeBbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][49]"
      ],
      "metadata": {
        "id": "bsWcKYXsoxum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][49][\"image\"]"
      ],
      "metadata": {
        "id": "dKEbLMAqricS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import string\n",
        "#@title Let's look at a sample in the dataset\n",
        "import math\n",
        "import json\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "# change this index from 0 to split size to see different samples\n",
        "sample = dataset['train'][49]\n",
        "image = sample['image']\n",
        "width, height = image.size\n",
        "print(f\"image width, height: {width, height}\")\n",
        "print(f\"prompt: {sample['prompt']}\")\n",
        "\n",
        "# bb = json.loads(sample[\"target_bounding_box\"])\n",
        "bb = sample[\"target_bounding_box\"]\n",
        "\n",
        "if isinstance(bb, str):\n",
        "  import ast\n",
        "  bb = ast.literal_eval(bb)\n",
        "\n",
        "print(f\"target bounding box: {bb}\")\n",
        "\n",
        "xmin = math.floor(width*bb[\"xmin\"])\n",
        "ymin = math.floor(height*bb[\"ymin\"])\n",
        "xmax = math.floor(width*bb[\"xmax\"])\n",
        "ymax = math.floor(height*bb[\"ymax\"])\n",
        "\n",
        "print(f\"to image pixel values: xmin, ymin, xmax, ymax: {xmin, ymin, xmax, ymax}\")\n",
        "\n",
        "shape = [(xmin, ymin), (xmax, ymax)]\n",
        "\n",
        "# create rectangle image\n",
        "img1 = ImageDraw.Draw(image)  \n",
        "img1.rectangle(shape, outline =\"green\", width=5)\n",
        "image.resize((int(width*0.5), int(height*0.5)))\n"
      ],
      "metadata": {
        "id": "6f2fjxGaHWli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Log to WANDB for EDA"
      ],
      "metadata": {
        "id": "y5VyhHmMli6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import wandb  \n",
        "# import ast\n",
        "# import numpy as np\n",
        "\n",
        "# cols = [\"prompt\", \"image\", \"width\", \"height\",\"xmin\", \"ymin\", \"xmax\", \"ymax\"]\n",
        "\n",
        "# class_id_to_label = {\n",
        "#     0: \"img\"\n",
        "# }\n",
        "\n",
        "# def get_data(idx):\n",
        "#   sample = dataset['train'][idx]\n",
        "#   prompt = sample['prompt']\n",
        "#   if isinstance(prompt, list): prompt = prompt[0]\n",
        "#   image = sample['image']\n",
        "#   if isinstance(image, list): image = image[0]\n",
        "#   width, height = image.size\n",
        "\n",
        "#   bb = sample[\"target_bounding_box\"]\n",
        "#   if isinstance(bb, list): bb = bb[0]\n",
        "#   if isinstance(bb, str): bb = ast.literal_eval(bb)\n",
        "\n",
        "#   return image, bb, prompt\n",
        "\n",
        "# n_imgs = 50\n",
        "# my_imgs = []\n",
        "\n",
        "# wandb.init(entity=\"morg\", project=\"ui_control\", job_type='eda')\n",
        "\n",
        "# for i in range(n_imgs):\n",
        "#   idx = np.random.randint(0, len(dataset[\"train\"]), 1)\n",
        "\n",
        "#   image, bb, prompt = get_data(idx)\n",
        "\n",
        "#   boxes = {\n",
        "#       \"ground_truth\": {\n",
        "#           \"box_data\" : [{\n",
        "#             \"position\": {\"minX\": bb[\"xmin\"], \"maxX\": bb[\"xmax\"], \"minY\": bb[\"ymin\"], \"maxY\": bb[\"ymax\"]},\n",
        "#             \"class_id\": 0,\n",
        "#             \"box_caption\": prompt,\n",
        "#             # \"domain\": \"pixel\"\n",
        "#           }],\n",
        "#           \"class_labels\": class_id_to_label\n",
        "#       }\n",
        "#   }\n",
        "\n",
        "#   my_imgs.append(wandb.Image(image, boxes=boxes, caption=f\"{idx}_{prompt}\"))\n",
        "\n",
        "#   # tbl = wandb.Table(columns=cols)\n",
        "\n",
        "# wandb.log({f\"bbox_eda\": my_imgs})\n",
        "\n",
        "# wandb.finish()"
      ],
      "metadata": {
        "id": "9-uFj0P7lgkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCjMK93Cz3zf"
      },
      "source": [
        "## Load model and processor\n",
        "\n",
        "Next, we load the model (which is an instance of [VisionEncoderDecoderModel](https://huggingface.co/docs/transformers/model_doc/vision-encoder-decoder), and the processor, which is the object that can be used to prepare inputs for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahkkeo8_o69z"
      },
      "outputs": [],
      "source": [
        "from transformers import VisionEncoderDecoderConfig\n",
        "# from transformers.src.transformers.models.vision_encoder_decoder import VisionEncoderDecoderConfig\n",
        "\n",
        "pretrained_repo_name = REFEXP_MODEL_CHECKPOINT\n",
        "pretrained_repo_branch = REFEXP_MODEL_CP_BRANCH\n",
        "\n",
        "max_length = 128\n",
        "image_size = [1280, 960]\n",
        "\n",
        "# update image_size of the encoder\n",
        "# during pre-training, a larger image size was used\n",
        "config = VisionEncoderDecoderConfig.from_pretrained(pretrained_repo_name, branch=pretrained_repo_branch)\n",
        "config.encoder.image_size = image_size # (height, width)\n",
        "# update max_length of the decoder (for generation)\n",
        "config.decoder.max_length = max_length\n",
        "# TODO we should actually update max_position_embeddings and interpolate the pre-trained ones:\n",
        "# https://github.com/clovaai/donut/blob/0acc65a85d140852b8d9928565f0f6b2d98dc088/donut/model.py#L602\n",
        "\n",
        "# Set the decoder_start_token_id\n",
        "# as per MBART: https://huggingface.co/facebook/mbart-large-50/blob/main/config.json\n",
        "config.decoder.decoder_start_token_id = 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# config"
      ],
      "metadata": {
        "id": "FPx2ts44RnSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84TkZP5zz4hE"
      },
      "outputs": [],
      "source": [
        "from transformers import DonutProcessor, VisionEncoderDecoderModel, BartConfig\n",
        "\n",
        "processor = DonutProcessor.from_pretrained(pretrained_repo_name, revision=pretrained_repo_branch)\n",
        "model = VisionEncoderDecoderModel.from_pretrained(pretrained_repo_name, revision=pretrained_repo_branch, config=config)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add special tokens\n",
        "\n",
        "For RefExp, we add special tokens for \\<prompt> \\<target_bounding_box>, \\<xmin>, \\<xmax>, \\<ymin> and \\<ymax>, to make sure that the model (actually the decoder) learns embedding vectors for those explicitly."
      ],
      "metadata": {
        "id": "PfTPbvNRCEDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "def add_tokens(list_of_tokens: List[str]):\n",
        "    \"\"\"\n",
        "    Add tokens to tokenizer and resize the token embeddings\n",
        "    \"\"\"\n",
        "    newly_added_num = processor.tokenizer.add_tokens(list_of_tokens)\n",
        "    if newly_added_num > 0:\n",
        "        model.decoder.resize_token_embeddings(len(processor.tokenizer))"
      ],
      "metadata": {
        "id": "CfJMb2o31AA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b46s3KR-x8Iv"
      },
      "source": [
        "## Create PyTorch dataset\n",
        "\n",
        "Here we create a regular PyTorch dataset.\n",
        "\n",
        "The model doesn't directly take the (image, JSON) pairs as input and labels. Rather, we create `pixel_values`, `decoder_input_ids` and `labels`. These are all PyTorch tensors. The `pixel_values` are the input images (resized, padded and normalized), the `decoder_input_ids` are the decoder inputs, and the `labels` are the decoder targets.\n",
        "\n",
        "The reason we create the `decoder_input_ids` explicitly here is because otherwise, the model would create them automatically based on the `labels` (by prepending the decoder start token ID, replacing -100 tokens by padding tokens). The reason for that is that we don't want the model to learn to generate the entire prompt, which includes the question. Rather, we only want it to learn to generate the answer. Hence, we'll set the labels of the prompt tokens to -100.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tWX_qJDvw_S"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "from typing import Any, List, Tuple\n",
        "import weakref\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "added_tokens = []\n",
        "\n",
        "class DonutDataset(Dataset):\n",
        "    \"\"\"\n",
        "    DonutDataset which is saved in huggingface datasets format. (see details in https://huggingface.co/docs/datasets)\n",
        "    Each row, consists of image blob, prompt and target bounding box.,\n",
        "    and it will be converted into input_tensor(vectorized image) and input_ids(tokenized string).\n",
        "    Args:\n",
        "        dataset_name_or_path: name of dataset (available at huggingface.co/datasets) or the path containing image files and metadata.jsonl\n",
        "        max_length: the max number of tokens for the target sequences\n",
        "        split: whether to load \"train\", \"validation\" or \"test\" split\n",
        "        ignore_id: ignore_index for torch.nn.CrossEntropyLoss\n",
        "        task_start_token: the special token to be fed to the decoder to conduct the target task\n",
        "        prompt_end_token: the special token at the end of the sequences\n",
        "        sort_json_key: whether or not to sort the JSON keys\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset_name_or_path: str,\n",
        "        max_length: int,\n",
        "        range_samples: int = None,\n",
        "        shuffle: bool = False,\n",
        "        split: str = \"train\",\n",
        "        ignore_id: int = -100,\n",
        "        task_start_token: str = \"<s>\",\n",
        "        prompt_end_token: str = None,\n",
        "        sort_json_key: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.max_length = max_length\n",
        "        self.split = split\n",
        "        self.ignore_id = ignore_id\n",
        "        self.task_start_token = task_start_token\n",
        "        self.prompt_end_token = prompt_end_token if prompt_end_token else task_start_token\n",
        "        self.sort_json_key = sort_json_key\n",
        "\n",
        "        self.dataset = load_dataset(dataset_name_or_path, split=self.split)\n",
        "\n",
        "        self.gt_token_sequences = []\n",
        "        if shuffle:\n",
        "          self.dataset = self.dataset.shuffle()\n",
        "        if range_samples is not None:\n",
        "          self.dataset = self.dataset.select(range_samples)\n",
        "        self.dataset = self.dataset.shuffle()\n",
        "        self.dataset_length = self.dataset.num_rows\n",
        "        # create an in-memory cache for pixel tensors\n",
        "        self.pixel_cache = weakref.WeakValueDictionary()\n",
        "        # create an in-memory cache for input_ids\n",
        "        self.input_ids_cache = weakref.WeakValueDictionary()\n",
        "        for sample in self.dataset:\n",
        "            prompt = sample[\"prompt\"]\n",
        "            # bb = json.loads(sample[\"target_bounding_box\"])\n",
        "            bb = sample[\"target_bounding_box\"]\n",
        "            # Trim float precision to simplify training with shorter string representations of component coordinates.\n",
        "            # 2 decimals precision seems to be a good balance between component position acccuracy and model convergance time.\n",
        "            # 3 decimals precision is good enough for screenshot size up to [1000x1000], but it takes longer for the model to converge.\n",
        "            # For even finer granurality, we cam increase precision to 4 for [10,000 x 10,000] screen sizes, but it will take much more training time and compute resources to converge.\n",
        "            if isinstance(bb, str): bb = ast.literal_eval(bb)\n",
        "            for key, value in bb.items():\n",
        "              bb[key] = round(value,2)\n",
        "\n",
        "            assert isinstance(bb, dict)\n",
        "            ground_truth = {\"prompt\": prompt, \"target_bounding_box\": bb}\n",
        "            gt_json = ground_truth\n",
        "\n",
        "            j2t = self.json2token(\n",
        "                  gt_json,\n",
        "                  update_special_tokens_for_json_key=self.split == \"train\",\n",
        "                  sort_json_key=self.sort_json_key,\n",
        "              ) + processor.tokenizer.eos_token\n",
        "            self.gt_token_sequences.append(j2t)\n",
        "\n",
        "        self.add_tokens([self.task_start_token, self.prompt_end_token])\n",
        "        self.prompt_end_token_id = processor.tokenizer.convert_tokens_to_ids(self.prompt_end_token)\n",
        "\n",
        "    def json2token(self, obj: Any, update_special_tokens_for_json_key: bool = True, sort_json_key: bool = True):\n",
        "        \"\"\"\n",
        "        Convert an ordered JSON object into a token sequence\n",
        "        \"\"\"\n",
        "        if type(obj) == dict:\n",
        "            if len(obj) == 1 and \"text_sequence\" in obj:\n",
        "                return obj[\"text_sequence\"]\n",
        "            else:\n",
        "                output = \"\"\n",
        "                if sort_json_key:\n",
        "                    keys = sorted(obj.keys(), reverse=True)\n",
        "                else:\n",
        "                    keys = obj.keys()\n",
        "                for k in keys:\n",
        "                    if update_special_tokens_for_json_key:\n",
        "                        self.add_tokens([fr\"<s_{k}>\", fr\"</s_{k}>\"])\n",
        "                    output += (\n",
        "                        fr\"<s_{k}>\"\n",
        "                        + self.json2token(obj[k], update_special_tokens_for_json_key, sort_json_key)\n",
        "                        + fr\"</s_{k}>\"\n",
        "                    )\n",
        "                return output\n",
        "        elif type(obj) == list:\n",
        "            return r\"<sep/>\".join(\n",
        "                [self.json2token(item, update_special_tokens_for_json_key, sort_json_key) for item in obj]\n",
        "            )\n",
        "        else:\n",
        "            obj = str(obj)\n",
        "            if f\"<{obj}/>\" in added_tokens:\n",
        "                obj = f\"<{obj}/>\"  # for categorical special tokens\n",
        "            return obj\n",
        "    \n",
        "    def add_tokens(self, list_of_tokens: List[str]):\n",
        "        \"\"\"\n",
        "        Add special tokens to tokenizer and resize the token embeddings of the decoder\n",
        "        \"\"\"\n",
        "        newly_added_num = processor.tokenizer.add_tokens(list_of_tokens)\n",
        "        if newly_added_num > 0:\n",
        "            model.decoder.resize_token_embeddings(len(processor.tokenizer))\n",
        "            added_tokens.extend(list_of_tokens)\n",
        "    \n",
        "    def __len__(self) -> int:\n",
        "        return self.dataset_length - 1\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Load image from image_path of given dataset_path and convert into input_tensor and labels\n",
        "        Convert gt data into input_ids (tokenized string)\n",
        "        Returns:\n",
        "            input_tensor : preprocessed image\n",
        "            input_ids : tokenized gt_data\n",
        "            labels : masked labels (model doesn't need to predict prompt and pad token)\n",
        "        \"\"\"\n",
        "        sample = self.dataset[idx]\n",
        "\n",
        "        # input_tensor\n",
        "        input_tensor = self.pixel_cache.get(idx)\n",
        "        if input_tensor is None:\n",
        "          pixel_values = processor(sample[\"image\"].convert(\"RGB\"), random_padding=self.split == \"train\", return_tensors=\"pt\").pixel_values\n",
        "          input_tensor = pixel_values.squeeze()\n",
        "          self.pixel_cache[idx] = input_tensor\n",
        "        # elif idx % 100 == 0:\n",
        "        #   print(f'{self.split} dataloader pixel_cache hit at index {idx}')\n",
        "\n",
        "        # input_ids\n",
        "        input_ids = self.input_ids_cache.get(idx)\n",
        "        if input_ids is None:\n",
        "          processed_parse = self.gt_token_sequences[idx]\n",
        "          input_ids = processor.tokenizer(\n",
        "              processed_parse,\n",
        "              add_special_tokens=False,\n",
        "              max_length=self.max_length,\n",
        "              padding=\"max_length\",\n",
        "              truncation=True,\n",
        "              return_tensors=\"pt\",\n",
        "          )[\"input_ids\"].squeeze(0)\n",
        "          self.input_ids_cache[idx] = input_ids\n",
        "        # elif idx % 100 == 0:\n",
        "        #   print(f'{self.split} dataloader input_ids cache hit at index {idx}')\n",
        "\n",
        "\n",
        "        if idx % 200 == 0:\n",
        "          print(f\"sameple #{idx}, input_ids: {input_ids}\")\n",
        "\n",
        "        if self.split == \"train\":\n",
        "            labels = input_ids.clone()\n",
        "            labels[\n",
        "                labels == processor.tokenizer.pad_token_id\n",
        "            ] = self.ignore_id  # model doesn't need to predict pad token\n",
        "            labels[\n",
        "                : torch.nonzero(labels == self.prompt_end_token_id).sum() + 1\n",
        "            ] = self.ignore_id  # model doesn't need to predict prompt (for VQA)\n",
        "            return input_tensor, input_ids, labels\n",
        "        else:\n",
        "            prompt_end_index = torch.nonzero(\n",
        "                input_ids == self.prompt_end_token_id\n",
        "            ).sum()  # return prompt end index instead of target output labels\n",
        "            return input_tensor, input_ids, prompt_end_index, processed_parse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_h6nyTm3RN0"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JpazNkf8CnA"
      },
      "outputs": [],
      "source": [
        "# we update some settings which differ from pretraining; namely the size of the images + no rotation required\n",
        "# source: https://github.com/clovaai/donut/blob/master/config/train_cord.yaml\n",
        "processor.feature_extractor.size = image_size[::-1] # should be (width, height)\n",
        "processor.feature_extractor.do_align_long_axis = False\n",
        "\n",
        "# For warm up phase, consider picking only a small subset to see if the model converges on the data\n",
        "max_train_samples = 1024\n",
        "# pick a range for sampling\n",
        "# range_train_samples = range(4000, 4000+max_train_samples)\n",
        "range_train_samples = range(max_train_samples)\n",
        "\n",
        "train_dataset = DonutDataset(REFEXP_DATASET_NAME, max_length=max_length, \n",
        "                            #  range_samples=range_train_samples,\n",
        "                             shuffle=True,\n",
        "                             split=\"train\", task_start_token=\"<s_refexp>\", prompt_end_token=\"<s_target_bounding_box>\",\n",
        "                             sort_json_key=False,\n",
        "                             )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]\n",
        "\n"
      ],
      "metadata": {
        "id": "vlrKXSzLBAwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset "
      ],
      "metadata": {
        "id": "0kwmbt8UypRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pick a small subset for initial val set to see if validation metrics improve\n",
        "max_val_samples = 1500 if len(dataset[\"validation\"]) > 1500 else len(dataset[\"validation\"])\n",
        "range_val_samples = range(0,max_val_samples,5)\n",
        "\n",
        "val_dataset = DonutDataset(REFEXP_DATASET_NAME, max_length=max_length, \n",
        "                          #  range_samples=range_val_samples,\n",
        "                             split=\"validation\", task_start_token=\"<s_refexp>\", prompt_end_token=\"<s_target_bounding_box>\",\n",
        "                             sort_json_key=False,\n",
        "                             )\n"
      ],
      "metadata": {
        "id": "cxv4RS8i-rsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pixel_values, decoder_input_ids, labels = train_dataset[0]\n"
      ],
      "metadata": {
        "id": "9ZUvmOdAs7xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pixel_values.shape"
      ],
      "metadata": {
        "id": "AQMuNYnA4XYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(labels)"
      ],
      "metadata": {
        "id": "vWKlLJML4o-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for decoder_input_id, label in zip(decoder_input_ids.tolist()[:-1], labels.tolist()[1:]):\n",
        "  if label != -100:\n",
        "    print(processor.decode([decoder_input_id]), processor.decode([label]))\n",
        "  else:\n",
        "    print(processor.decode([decoder_input_id]), label)"
      ],
      "metadata": {
        "id": "Zud4yPeN4qQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pixel_values, decoder_input_ids, prompt_end_index, processed_parse = val_dataset[0]\n"
      ],
      "metadata": {
        "id": "0xcQqFDsBmPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pixel_values.shape"
      ],
      "metadata": {
        "id": "Szz2rquaBq89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_end_index"
      ],
      "metadata": {
        "id": "1mUwVF9yBr_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_parse"
      ],
      "metadata": {
        "id": "cj2gybmeBuvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygTIylugfasG"
      },
      "source": [
        "## Create PyTorch DataLoaders\n",
        "\n",
        "Next, we create corresponding PyTorch DataLoaders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLQ_Vl5MLugu"
      },
      "outputs": [],
      "source": [
        "print(f\"train dataset length: {train_dataset.dataset_length}\")\n",
        "print(f\"validation dataset length: {val_dataset.dataset_length}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "9BFgvT3twpaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Set optimal batch size for training and validation \n",
        "# Currently there is an issue with VisualEncoderDecoder when batch size > 1\n",
        "# Causes error in loss calculation during training\n",
        "train_batch_size = 2 # Usually increments of 8. Value depends on GPU capacity.\n",
        "print(f\"train_batch_size: {train_batch_size}\")\n",
        "val_batch_size = 8"
      ],
      "metadata": {
        "id": "lSwRsj8imFuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)"
      ],
      "metadata": {
        "id": "pIkar2gaX4Xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxtTVgNnfdkD"
      },
      "source": [
        "Let's verify a batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHurHlLnL8Xm"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "pixel_values, decoder_input_ids, labels = batch\n",
        "print(pixel_values.shape, decoder_input_ids.shape, labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels.view(-1).shape"
      ],
      "metadata": {
        "id": "wS6vd2YHgrg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels.reshape(-1).shape"
      ],
      "metadata": {
        "id": "wut7AzxzhNN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_input_ids.shape"
      ],
      "metadata": {
        "id": "Vo0TXXDL8oHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can clearly see that we have set the labels of all prompt tokens (which includes the prompt) to -100, to make sure the model doesn't learn to generate them. We only start to have labels starting from the \\<s_target_bounding_box> decoder input token."
      ],
      "metadata": {
        "id": "a_GvAiCQkPSf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8ehAwgPZrcc"
      },
      "outputs": [],
      "source": [
        "for decoder_input_id, label in zip(decoder_input_ids[0].tolist()[:-1][:50], labels[0].tolist()[1:][:50]):\n",
        "  if label != -100:\n",
        "    print(processor.decode([decoder_input_id]), processor.decode([label]))\n",
        "  else:\n",
        "    print(processor.decode([decoder_input_id]), label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnmD7rRy2WLI"
      },
      "source": [
        "## Define LightningModule\n",
        "\n",
        "We'll fine-tune the model using [PyTorch Lightning](https://www.pytorchlightning.ai/) here, but note that you can of course also just fine-tune with regular PyTorch, HuggingFace [Accelerate](https://github.com/huggingface/accelerate), the HuggingFace [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer), etc.\n",
        "\n",
        "PyTorch Lightning is pretty convenient to handle things like device placement, mixed precision and logging for you."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining Evaluation Metric\n",
        "\n",
        "The pretrained Donut DocVQA model that we started with uses Edit Distance, which is reasonable to measure how close the answer text is close to the ground truth. However edit distance is not the most natural choice for bounding box evaluation.\n",
        "\n",
        "Edit distance does reward perfect match, but it may show less useful intermediate values. For example `xmin=0.123` and `xmin=0.923` are only 1 character separated, but in terms of bounding box overlap, they are very far apart.\n",
        "\n",
        "A popular metric used in object detection is [mAP](https://towardsdatascience.com/on-object-detection-metrics-with-worked-example-216f173ed31e). It takes into account IoU of bounding boxes and detected object classes. It treats both object class and bounding box as classification problems which is useful technically but not very intuitive. One of the issues with mAP is that it does not give us any signal whether the model is improving bounding box detection until it reaches a certain threshold (usually 0.5 IoU).\n",
        "\n",
        "Can we define an evaluation metric that is both useful and intuitive for the RefExp task? Ideally we would like it to give us a continous feedback wheter the model is advancing on a stable trajectory towards perfect accuracy. Ideally it would not have blackout regions during training where we are not certain if the product is improving incrementally.\n"
      ],
      "metadata": {
        "id": "UXzLACRy1ZLP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Distance between rectangle centers\n",
        "\n",
        "One intuitive metric could be to measure the distance in bbox center coordinates between prediction and ground truth.\n",
        "\n",
        "In the early stages of training, center distance is a useful coarse grained eval metric. It tells us how close the center of the predicted bounding box is from the center of the ground truth bounding box.\n",
        "\n",
        "As the model improves, we can switch to a more fine grained eval metric such as IoU or CUI. See further details below.\n"
      ],
      "metadata": {
        "id": "PWQWMb2fH2V1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def validate_bbox(bb):\n",
        "  \"\"\"\n",
        "  Ensures correct coordinates for bounding box. Returns true\n",
        "  Returns\n",
        "  -------\n",
        "  True if bbox coordinates are valid. False otherwise.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    if bb['xmin'] > bb['xmax']:\n",
        "      return False\n",
        "    if bb['ymin'] > bb['ymax']:\n",
        "      return False\n",
        "  except Exception as e:\n",
        "    print(f\"Invalid bbox: {bb}\", e)\n",
        "    return False \n",
        "  return True\n",
        "\n",
        "def get_center_distance(bb1, bb2):\n",
        "    \"\"\"\n",
        "    Calculate the distance between the centers of two bounding boxes.\n",
        "    Best case, distance between centers of predicted and ground truth bounding boxes will be 0.\n",
        "    Worst case,  distance will be the larges diagonal in the screen - sqrt(1,1).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    bb1 : dict\n",
        "        Keys: {'xmin', 'xmax', 'ymin', 'ymax'}\n",
        "        The (xmin, ymin) position is at the top left corner,\n",
        "        the (xmax, y2) position is at the bottom right corner\n",
        "    bb2 : dict\n",
        "        Keys: {'xmin', 'xmax', 'ymin', 'ymax'}\n",
        "        The (x, y) position is at the top left corner,\n",
        "        the (xmax, ymax) position is at the bottom right corner\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        in [0, sqrt(1+1)]\n",
        "    \"\"\"\n",
        "    best_case = 0.0\n",
        "    worst_case = math.sqrt(1+1) # max diagonal\n",
        "    if not validate_bbox(bb1) or not validate_bbox(bb2):\n",
        "      return worst_case\n",
        "\n",
        "    # determine the coordinates of the center of each rectangle\n",
        "    bb1_x_center = (bb1['xmax'] + bb1['xmin'])/2\n",
        "    bb1_y_center = (bb1['ymax'] + bb1['ymin'])/2\n",
        "\n",
        "    bb2_x_center = (bb2['xmax'] + bb2['xmin'])/2\n",
        "    bb2_y_center = (bb2['ymax'] + bb2['ymin'])/2\n",
        "    center_dist = math.sqrt((bb2_x_center - bb1_x_center)**2 + (bb2_y_center - bb1_y_center)**2)\n",
        "\n",
        "    assert center_dist >= best_case\n",
        "    assert center_dist <= worst_case # sqrt(1+1)\n",
        "    return center_dist"
      ],
      "metadata": {
        "id": "YYk7Y6SoU9Qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Intersection over Union\n",
        "\n",
        "We can use [Intersection over Union](https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/) (IoU) to measure bounding box match as a validation progress metric instead of the Edit Distance metric used in DocVQA.\n",
        "\n",
        "Since the model outputs bounding box coordinates, we would like to see these bounding boxes trend towards overlapping exactly with the ground truth.\n",
        "\n",
        "![IoU image](https://i.stack.imgur.com/n1AZj.png)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RKtTK9RWU9kH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_iou(bb1, bb2):\n",
        "    \"\"\"\n",
        "    Calculate the Intersection over Union (IoU) of two bounding boxes.\n",
        "    Best case, IoU is 1 indicating perfect match between prediction and ground truth.\n",
        "    Worst case, IoU is 0 when no overlap between bounding boxes.\n",
        "    Modifed version from the following original on stackoverflow:\n",
        "    https://stackoverflow.com/questions/25349178/calculating-percentage-of-bounding-box-overlap-for-image-detector-evaluation\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    bb1 : dict\n",
        "        Keys: {'xmin', 'xmax', 'ymin', 'ymax'}\n",
        "        The (xmin, ymin) position is at the top left corner,\n",
        "        the (xmax, y2) position is at the bottom right corner\n",
        "    bb2 : dict\n",
        "        Keys: {'xmin', 'xmax', 'ymin', 'ymax'}\n",
        "        The (x, y) position is at the top left corner,\n",
        "        the (xmax, ymax) position is at the bottom right corner\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        in [0, 1]\n",
        "    \"\"\"\n",
        "    best_case = 1.0\n",
        "    worst_case = 0.0\n",
        "    if not validate_bbox(bb1) or not validate_bbox(bb2):\n",
        "      return worst_case\n",
        "\n",
        "    # determine the coordinates of the intersection rectangle\n",
        "    x_left = max(bb1['xmin'], bb2['xmin'])\n",
        "    y_top = max(bb1['ymin'], bb2['ymin'])\n",
        "    x_right = min(bb1['xmax'], bb2['xmax'])\n",
        "    y_bottom = min(bb1['ymax'], bb2['ymax'])\n",
        "\n",
        "    # print(f\"IoU x_left: {x_left}, y_top: {y_top}, x_right: {x_right}, y_bottom: {y_bottom}\")\n",
        "\n",
        "    if x_right < x_left or y_bottom < y_top:\n",
        "        return worst_case # no bbox overlap\n",
        "\n",
        "    # The intersection of two axis-aligned bounding boxes is always an\n",
        "    # axis-aligned bounding box\n",
        "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
        "    # print(f\"IoU intersection_area: {intersection_area}\")\n",
        "\n",
        "    # compute the area of both AABBs\n",
        "    bb1_area = (bb1['xmax'] - bb1['xmin']) * (bb1['ymax'] - bb1['ymin'])\n",
        "    bb2_area = (bb2['xmax'] - bb2['xmin']) * (bb2['ymax'] - bb2['ymin'])\n",
        "    # print(f\"IoU bb1_area: {bb1_area}\")\n",
        "    # print(f\"IoU bb2_area: {bb2_area}\")\n",
        "\n",
        "    # compute the intersection over union by taking the intersection\n",
        "    # area and dividing it by the sum of prediction + ground-truth\n",
        "    # areas - the interesection area\n",
        "    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n",
        "    # if iou > 0:\n",
        "    #   print(f\"IoU input bb1, bb2: {bb1} , {bb2}\")\n",
        "    #   print(f\"IoU : {iou}\")\n",
        "    assert iou >= worst_case\n",
        "    assert iou <= best_case\n",
        "    return iou"
      ],
      "metadata": {
        "id": "AmDmvkkfnDef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Center_Distance * (Union - Intersection)\n",
        "\n",
        "Central distance is convenient in the early stages of training but it does not tell us if two bounding boxes match. It is possible centers to be exatcly the same but corners to be far apart.\n",
        "\n",
        "IoU on the other hand has the issue of a long flat 0 range before two bouning boxes have at least a few overlapping pixels. It does not reward model performance in the stage when bounding boxes do not overlap but are pulled closer to each other.\n",
        "\n",
        "What if we measure the difference between Union and Intersection and multiply by the center distance. We end up with a function that is non-zero for all cases except complete overlap between two bounding boxes. It is also monotonously decreasing as the centers of two bounding boxes come close to each other. The function can be 0 if both boxes have 0 surface area, but that is not a realistic nor useful sample to work with so we will ignore such outliers.\n",
        "\n",
        "The center distance component gives us a way to measure model improvements while bounding boxes do not intersect at all but their centers come closer.\n",
        "\n",
        "It also rewards bounding box intersection.\n",
        "\n",
        "We will look to minimize this function.\n",
        "\n",
        "![CUI](https://user-images.githubusercontent.com/2234901/215337428-ef2f30f4-6a51-4ea4-a4fc-61e54b593fe9.png)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wZGE3iK2jYxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_cui(bb1, bb2):\n",
        "    \"\"\"\n",
        "    Calculates Central Distance times Union minus Intersection.\n",
        "    The model should aim to minimize this function towards 0,\n",
        "    which is complete bounding box overlap.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    bb1 : dict\n",
        "        Keys: {'xmin', 'xmax', 'ymin', 'ymax'}\n",
        "        The (xmin, ymin) position is at the top left corner,\n",
        "        the (xmax, y2) position is at the bottom right corner\n",
        "    bb2 : dict\n",
        "        Keys: {'xmin', 'xmax', 'ymin', 'ymax'}\n",
        "        The (x, y) position is at the top left corner,\n",
        "        the (xmax, ymax) position is at the bottom right corner\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        in [0, 1]\n",
        "    \"\"\"\n",
        "    best_case = 0.0\n",
        "    # max(central distance) = sqrt(1+1)\n",
        "    # max(U-I) = 1\n",
        "    worst_case = 1*math.sqrt(1+1) \n",
        "    # print(f\"\\n\\n>>>>>>>> get_cui bb1: {bb1}, bb2 {bb2} \\n\\n\")\n",
        "    if not validate_bbox(bb1) or not validate_bbox(bb2):\n",
        "      # print(f\"get_cui: invalid boundig box: {bb1}, {bb2}\")\n",
        "      return worst_case\n",
        "\n",
        "    # determine the coordinates of the intersection rectangle\n",
        "    x_left = max(bb1['xmin'], bb2['xmin'])\n",
        "    y_top = max(bb1['ymin'], bb2['ymin'])\n",
        "    x_right = min(bb1['xmax'], bb2['xmax'])\n",
        "    y_bottom = min(bb1['ymax'], bb2['ymax'])\n",
        "\n",
        "    # print(f\"get_cui: x_left: {x_left}, y_top: {y_top}, x_right: {x_right}, y_bottom: {y_bottom}\")\n",
        "\n",
        "\n",
        "    if x_right > x_left and y_bottom > y_top:\n",
        "      # The intersection of two axis-aligned bounding boxes is always an\n",
        "      # axis-aligned bounding box\n",
        "      intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
        "    else:\n",
        "      intersection_area = 0.0\n",
        "    # print(f\"get_cui: intersection_area: {intersection_area}\")\n",
        "\n",
        "\n",
        "    # compute Union: the area of both AABBs\n",
        "    bb1_area = (bb1['xmax'] - bb1['xmin']) * (bb1['ymax'] - bb1['ymin'])\n",
        "    bb2_area = (bb2['xmax'] - bb2['xmin']) * (bb2['ymax'] - bb2['ymin'])\n",
        "    union_area = float(bb1_area + bb2_area - intersection_area)\n",
        "    # print(f\"get_cui: bb1_area: {bb1_area}\")\n",
        "    # print(f\"get_cui: bb2_area: {bb2_area}\")\n",
        "    # print(f\"get_cui: union_area: {union_area}\")\n",
        "\n",
        "    # compute the intersection over union by taking the intersection\n",
        "    # area and dividing it by the sum of prediction + ground-truth\n",
        "    # areas - the interesection area\n",
        "    non_overlapping_area = float(union_area - intersection_area)\n",
        "    # print(f\"get_cui: non_overlapping_area: {non_overlapping_area}\")\n",
        "    center_distance = get_center_distance(bb1, bb2)\n",
        "    # print(f\"get_cui: center_distance: {center_distance}\")\n",
        "    cui = non_overlapping_area*center_distance\n",
        "    # print(f\"get_cui: CUI score: {cui}\")\n",
        "    intersection_area\n",
        "    assert cui >= best_case\n",
        "    assert cui <= worst_case\n",
        "    return cui"
      ],
      "metadata": {
        "id": "SJhf6dgvjQpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# is_valid = validate_bbox({'xmin': 0.04, 'ymin': 0.68, 'xmax': 0.48, 'ymax': 0.72})\n",
        "# print(is_valid)"
      ],
      "metadata": {
        "id": "EdwzQ0_Nzj8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# get_cui({'xmin': 0.04, 'ymin': 0.68, 'xmax': 0.48, 'ymax': 0.72},\n",
        "#       {'xmin': 0.50, 'ymin': 0.68, 'xmax': 0.58, 'ymax': 0.73})"
      ],
      "metadata": {
        "id": "Cqk3K0ibzDHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Donut Lightning Module"
      ],
      "metadata": {
        "id": "2KxW79QQIMc5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRm5i4gWG-sb"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "# from nltk import edit_distance\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.utilities import rank_zero_only\n",
        "\n",
        "\n",
        "class DonutModelPLModule(pl.LightningModule):\n",
        "    def __init__(self, config, processor, model):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.processor = processor\n",
        "        self.model = model\n",
        "        self.batch_size = train_batch_size\n",
        "        self.learning_rate = self.config.get(\"lr\")        \n",
        "        # self.log_dict(config)\n",
        "        self.save_hyperparameters(ignore=['model'])\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        pixel_values, decoder_input_ids, labels = batch\n",
        "        \n",
        "        # print(f\"LABELS labels SHAPE: {labels.shape}\")\n",
        "        # print(f\"LABELS labels: {labels[0, :][:10]}\")\n",
        "        # print(f\"LABELS labels[:, 1:] SHAPE: {labels[:, 1:].shape}\")\n",
        "        # print(f\"LABELS labels[:, 1:]: {labels[0, 1:][:10]}\")\n",
        "\n",
        "        # # outputs = self.model(pixel_values,\n",
        "        # #                      decoder_input_ids=decoder_input_ids[:, :-1],\n",
        "        # #                      labels=labels[:, 1:])\n",
        "\n",
        "        # lbls = labels[:, 1:].clone()\n",
        "        outputs = self.model(pixel_values,\n",
        "                             decoder_input_ids=decoder_input_ids[:, :-1],\n",
        "                             labels=labels[:, 1:])\n",
        "        \n",
        "        loss = outputs.loss\n",
        "        # self.log_dict({\"train/train_loss\": loss}, sync_dist=True)\n",
        "        self.log(\"train/train_loss\", loss, on_step=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def token2bbox(self, seq: str):\n",
        "        target_bbox = self.processor.token2json(seq)\n",
        "        bbox = target_bbox.get('target_bounding_box')\n",
        "        if bbox is None:\n",
        "          print(f\"token2bbox seq has no target_bounding_box, seq:{seq}\")\n",
        "          bbox = bbox = {\"xmin\": 0, \"ymin\": 0, \"xmax\": 0, \"ymax\": 0}\n",
        "          return bbox\n",
        "        # print(f\"token2 bounding box json: {bbox}\")\n",
        "        # safeguard in case text prediction is missing some bounding box coordinates\n",
        "        # or coordinates are not valid numeric values\n",
        "        try:\n",
        "          xmin = float(bbox.get(\"xmin\", 0))\n",
        "        except Exception:\n",
        "          xmin = 0\n",
        "        try:\n",
        "          ymin = float(bbox.get(\"ymin\", 0))\n",
        "        except Exception:\n",
        "          ymin = 0\n",
        "        try:\n",
        "          xmax = float(bbox.get(\"xmax\", 1))\n",
        "        except Exception:\n",
        "          xmax = 1\n",
        "        try:\n",
        "          ymax = float(bbox.get(\"ymax\", 1))\n",
        "        except Exception:\n",
        "          ymax = 1\n",
        "        # replace str with float coords\n",
        "        bbox = {\"xmin\": xmin, \"ymin\": ymin, \"xmax\": xmax, \"ymax\": ymax}\n",
        "        # print(f\"token2 bounding box float: {bbox}\")\n",
        "        return bbox\n",
        "\n",
        "    def create_wandb_bbox_image(self, answer_bbox, pred_bbox, answer, pixel_values):\n",
        "      class_id_to_label = {0: \"ground_truth\", 1:\"prediction\"}\n",
        "      caption = answer.split(\"</s_prompt>\")[0]\n",
        "      boxes = {\n",
        "          \"ground_truth\": {\n",
        "              \"box_data\" : [{\n",
        "                \"position\": {\"minX\": answer_bbox[\"xmin\"], \"maxX\": answer_bbox[\"xmax\"], \"minY\": answer_bbox[\"ymin\"], \"maxY\": answer_bbox[\"ymax\"]},\n",
        "                \"class_id\": 0,\n",
        "                \"box_caption\": caption,\n",
        "              }],\n",
        "              \"class_labels\": class_id_to_label\n",
        "          },\n",
        "          \"predictions\": {\n",
        "              \"box_data\" : [{\n",
        "                \"position\": {\"minX\": pred_bbox[\"xmin\"], \"maxX\": pred_bbox[\"xmax\"], \"minY\": pred_bbox[\"ymin\"], \"maxY\": pred_bbox[\"ymax\"]},\n",
        "                \"class_id\": 1,\n",
        "                # \"box_caption\": prompt,\n",
        "              }],\n",
        "              \"class_labels\": class_id_to_label\n",
        "          }\n",
        "        }\n",
        "\n",
        "      return wandb.Image(pixel_values[0], boxes=boxes, caption=f\"{caption}\")\n",
        "\n",
        "    def validation_step(self, batch, batch_idx, dataset_idx=0):\n",
        "        pixel_values, decoder_input_ids, prompt_end_idxs, answers = batch\n",
        "        decoder_prompts = pad_sequence(\n",
        "            [input_id[: end_idx + 1] for input_id, end_idx in zip(decoder_input_ids, prompt_end_idxs)],\n",
        "            batch_first=True,\n",
        "        )\n",
        "        \n",
        "        outputs = self.model.generate(pixel_values,\n",
        "                                   decoder_input_ids=decoder_prompts,\n",
        "                                   max_length=max_length,\n",
        "                                   early_stopping=True,\n",
        "                                   pad_token_id=self.processor.tokenizer.pad_token_id,\n",
        "                                   eos_token_id=self.processor.tokenizer.eos_token_id,\n",
        "                                   use_cache=True,\n",
        "                                   num_beams=1,\n",
        "                                   bad_words_ids=[[self.processor.tokenizer.unk_token_id]],\n",
        "                                   return_dict_in_generate=True,)\n",
        "    \n",
        "        predictions = []\n",
        "        for seq in self.processor.tokenizer.batch_decode(outputs.sequences):\n",
        "            seq = seq.replace(self.processor.tokenizer.eos_token, \"\").replace(self.processor.tokenizer.pad_token, \"\")\n",
        "            seq = re.sub(r\"<.*?>\", \"\", seq, count=1).strip()  # remove first task start token\n",
        "            predictions.append(seq)\n",
        "\n",
        "        # scores = list()\n",
        "        scores = {\"iou\":[], \"cui\":[], \"wandb_images\":[]}\n",
        "        for pred, answer in zip(predictions, answers):\n",
        "            answer = re.sub(r\"<.*?>\", \"\", answer, count=1)\n",
        "            answer = answer.replace(self.processor.tokenizer.eos_token, \"\")\n",
        "            answer_bbox = self.token2bbox(answer)\n",
        "            pred_bbox = self.token2bbox(pred)\n",
        "            # scores.append(get_center_distance(pred_bbox, answer_bbox))\n",
        "            #scores.append(get_iou(pred_bbox, answer_bbox))\n",
        "            # scores.append(get_cui(pred_bbox, answer_bbox))\n",
        "            cui = get_cui(pred_bbox, answer_bbox)\n",
        "            scores[\"cui\"].append(get_cui(pred_bbox, answer_bbox))\n",
        "\n",
        "            iou = get_iou(pred_bbox, answer_bbox)\n",
        "            scores[\"iou\"].append(get_iou(pred_bbox, answer_bbox))\n",
        "\n",
        "            # Create wandb images with bounding boxes\n",
        "            wandb_img = self.create_wandb_bbox_image(answer_bbox, pred_bbox, answer, pixel_values)\n",
        "            if len(scores[\"wandb_images\"]) <= 16: scores[\"wandb_images\"].append(wandb_img)\n",
        "            else: scores[\"wandb_images\"].append(0)\n",
        "\n",
        "            if self.config[\"verbose\"] == True:   #and len(scores) == 1:\n",
        "              print(f\"      Prediction: {pred}\")\n",
        "              print(f\"          Answer: {answer}\")\n",
        "              print(f\" Prediction bbox: {pred_bbox}\")\n",
        "              print(f\"     Answer bbox: {answer_bbox}\")\n",
        "              # print(f\"Eval score (Center Distance): {scores[0]}\")\n",
        "              print(f\"Eval score CUI=CDx(U-I): {cui}\")\n",
        "              # iou = get_iou(pred_bbox, answer_bbox)\n",
        "              print(f\"Eval score (IoU): {iou}\")\n",
        "              # print(f\"Eval score (IoU): {scores[0]}\")\n",
        "              # print(f\"Eval score (Edit Distance): {scores[2]}\")\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def validation_epoch_end(self, validation_step_outputs):\n",
        "        # I set this to 1 manually\n",
        "        # (previously set to len(self.config.dataset_name_or_paths))\n",
        "        num_of_loaders = 1\n",
        "        if num_of_loaders == 1:\n",
        "            validation_step_outputs = [validation_step_outputs]\n",
        "        assert len(validation_step_outputs) == num_of_loaders\n",
        "        \n",
        "        val_metric = [{\"cui\": 0, \"iou\":0}] * num_of_loaders\n",
        "        total_metric = [{\"cui\": 0, \"iou\":0}] * num_of_loaders\n",
        "        cnt = [{\"cui\": 0, \"iou\": 0}] * num_of_loaders\n",
        "        wandb_imgs = [[]]\n",
        "\n",
        "        for i, dataset_output in enumerate(validation_step_outputs):\n",
        "          for m in [\"cui\", \"iou\"]:\n",
        "            for scores_dict in dataset_output:\n",
        "              scores = scores_dict[m]\n",
        "              for scr in scores:\n",
        "                cnt[i][m] += 1\n",
        "                total_metric[i][m] += np.sum(scr)\n",
        "              \n",
        "            val_metric[i][m] = total_metric[i][m] / cnt[i][m]\n",
        "          \n",
        "          for scores_dict in dataset_output:\n",
        "            imgs = scores_dict[\"wandb_images\"]\n",
        "            for im in imgs:\n",
        "              if not isinstance(im, int) and len(wandb_imgs[i]) <= 20: \n",
        "                wandb_imgs[i].append(im)\n",
        "\n",
        "          wandb.log({f\"val_predictions/preds_{i}th_dataset\": wandb_imgs[i]}, commit=False)\n",
        "          data_to_log = { \n",
        "              f\"val/iou_{i}\": val_metric[i][\"iou\"],\n",
        "              f\"val/cui_{i}\": val_metric[i][\"cui\"],\n",
        "              }\n",
        "\n",
        "          self.log_dict(data_to_log, sync_dist=True)\n",
        "\n",
        "\n",
        "        # # cnt = [0] * num_of_loaders\n",
        "        # # total_metric = [0] * num_of_loaders\n",
        "        # # val_metric = [0] * num_of_loaders\n",
        "\n",
        "        # metrics = {}\n",
        "        # for i, batches in enumerate(validation_step_outputs):\n",
        "        #   print(validation_step_outputs)\n",
        "        #   val_metrics = {\"cui\": 0, \"iou\":0}\n",
        "        #   for sample in batches:\n",
        "        #     for k in [\"cui\", \"iou\"]:\n",
        "        #       r = sample[k]\n",
        "        #       for scores in r:\n",
        "        #           cnt[i] += len(scores)\n",
        "        #           total_metric[i] += np.sum(scores)\n",
        "        #       val_metric[i] = total_metric[i] / cnt[i]\n",
        "              \n",
        "        #       # val_metric_name = f\"val/val_metric_{i}th_dataset\"\n",
        "        #       # val_metric_name = f\"val/cui_{i}th_dataset\"\n",
        "        #       metrics[f\"val/{k}_{i}th_dataset\"] = val_metric[i]\n",
        "            \n",
        "        #     # self.log_dict({val_metric_name: val_metric[i]}, sync_dist=True)\n",
        "        #   metrics[f\"val/preds_{i}th_dataset\"] = results[\"wandb_images\"][:20]\n",
        "        #   self.log_dict(metrics, sync_dist=True)\n",
        "        # # self.log_dict({\"val_metric\": np.sum(total_metric) / np.sum(cnt)}, sync_dist=True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html?highlight=configure_optimizers#configure-optimizers\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        # https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html\n",
        "        # we use max below, because we want the lr to decrease if IoU stops increasing\n",
        "        scheduler = ReduceLROnPlateau(optimizer, mode='max', patience=3) # previously patience=3, 5  \n",
        "        # log initial value for val_metric to avoid train error before its calculated\n",
        "        # self.log_dict({\"val_metric\": 0.0}, sync_dist=True)\n",
        "        return  {\n",
        "          \"optimizer\": optimizer,\n",
        "          \"lr_scheduler\": {\n",
        "              \"scheduler\":scheduler,\n",
        "              \"monitor\": \"val/cui_0\", # track IoU progress\n",
        "              # \"frequency\": \"indicates how often the metric is updated\"\n",
        "              # If \"monitor\" references validation metrics, then \"frequency\" should be set to a\n",
        "              # multiple of \"trainer.check_val_every_n_epoch\".\n",
        "        }\n",
        "    }\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return train_dataloader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return val_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we instantiate the module:"
      ],
      "metadata": {
        "id": "bKujfvIDlAHo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxNJhCGjKhtR"
      },
      "outputs": [],
      "source": [
        "# Since the whole dataset is too big to train in a single epoch\n",
        "# We will sample a small subset (5%-10%) per loop and train for a few epochs\n",
        "# Then sample again and loop a few more epochs\n",
        "# In effect simulating training on the whole dataset.\n",
        "max_epochs_per_loop=30 # previously at 30 epochs and 1024 training samples\n",
        "print(f\"max_epochs_per_loop: {max_epochs_per_loop}\")\n",
        "\n",
        "num_training_samples_per_epoch=800 # initially 800\n",
        "print(f\"num_training_samples_per_epoch: {num_training_samples_per_epoch}\")\n",
        "\n",
        "# Start at 3e-5 and reduce gradually every few epochs if loss oscilations too high. Use LR scheduler if epochs > 10.\n",
        "# See scheduler docs: https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html\n",
        "learning_rate= 3e-5 # previously , 1e-6, 1e-5\n",
        "print(f\"learning_rate: {learning_rate}\")\n",
        "\n",
        "# Aim for 10%. Examples: 20 = 800/8*2/10, 10%; 300 for 800/8*30/10\n",
        "warmup_steps=(num_training_samples_per_epoch/train_batch_size)*max_epochs_per_loop/10\n",
        "print(f\"warmup_steps: {warmup_steps}\")\n",
        "\n",
        "\n",
        "def getPLModuleConfig():\n",
        "  config = {\"max_epochs\": max_epochs_per_loop, # aim for 30,\n",
        "            \"val_check_interval\": 1.0, # how many times we want to validate during an epoch\n",
        "            \"check_val_every_n_epoch\":0.5,\n",
        "            \"gradient_clip_val\":1.0,\n",
        "            \"num_training_samples_per_epoch\": num_training_samples_per_epoch,\n",
        "            \"lr\": learning_rate,\n",
        "            \"train_batch_sizes\": [train_batch_size],\n",
        "            \"val_batch_sizes\": [val_batch_size],\n",
        "            # \"seed\":2022,\n",
        "            # \"num_nodes\": 1,\n",
        "            \"warmup_steps\": warmup_steps, \n",
        "            \"result_path\": \"./result\",\n",
        "            \"verbose\": False,\n",
        "            }\n",
        "  print(f'PL Module Config: {config}')\n",
        "  return config\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZoPiDOPKg0o"
      },
      "source": [
        "## Train!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "# clear any previously open logging session\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "GQhXL0AdWpuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiK6-vQHKnBy"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning.callbacks import StochasticWeightAveraging, LearningRateMonitor\n",
        "\n",
        "# wandb_logger = WandbLogger(entity=\"morg\", project=\"ui_control\")\n",
        "\n",
        "# Take advantage of A100 GPU features\n",
        "# https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
        "# torch.set_float32_matmul_precision('medium')\n",
        "\n",
        "\n",
        "def prep_trainer():\n",
        "  global processor, model, trainer, model_module\n",
        "  config = getPLModuleConfig()\n",
        "  model_module = DonutModelPLModule(config, processor, model)\n",
        "  wandb.finish() # flush any open wandb logging session\n",
        "  wandb_logger = WandbLogger(project=\"ui_control\")\n",
        "  # Take advantage of A100 GPU features\n",
        "  # https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
        "  torch.set_float32_matmul_precision('medium')\n",
        "\n",
        "  # log learning rate changes by optimizer\n",
        "  lr_monitor = LearningRateMonitor()\n",
        "\n",
        "  trainer = pl.Trainer(\n",
        "          accelerator=\"auto\",\n",
        "          devices=\"auto\",\n",
        "          auto_scale_batch_size=True,\n",
        "          auto_lr_find=True,\n",
        "          # accelerator=\"gpu\",\n",
        "          # devices=1,\n",
        "          max_epochs=config.get(\"max_epochs\"),\n",
        "          val_check_interval=config.get(\"val_check_interval\"),\n",
        "          check_val_every_n_epoch=config.get(\"check_val_every_n_epoch\"),\n",
        "          gradient_clip_val=config.get(\"gradient_clip_val\"),\n",
        "          precision=16, # we'll use mixed precision\n",
        "          num_sanity_val_steps=0,\n",
        "          logger=wandb_logger,\n",
        "          log_every_n_steps=2,\n",
        "          benchmark=True, # usually speeds up training,\n",
        "          accumulate_grad_batches=16,\n",
        "          # strategy=\"ddp_notebook\",\n",
        "          # Other effective optimization techniques follow\n",
        "          # https://pytorch-lightning.readthedocs.io/en/stable/advanced/training_tricks.html#accumulate-gradients\n",
        "          # https://pytorch-lightning.readthedocs.io/en/stable/advanced/training_tricks.html#stochastic-weight-averaging\n",
        "          # accumulate_grad_batches={0: 8}, # , 3: 4, 6: 8, 9: 4, 12: 2, 15: 1},\n",
        "          callbacks=[lr_monitor], # , StochasticWeightAveraging(swa_lrs=1e-5)]\n",
        "          # strategy=\"ddp_notebook\",\n",
        "          # callbacks=[lr_callback, checkpoint_callback],\n",
        "  )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prep_trainer()\n",
        "trainer.fit(model_module) # , ckpt_path=\"last\")"
      ],
      "metadata": {
        "id": "KNax96JP0CWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Push to hub and reuse\n",
        "\n",
        "HuggingFace's [hub](https://huggingface.co/) is a nice place to host, version and share machine learning models (and datasets, and demos in the form of [Spaces](https://huggingface.co/spaces)).\n",
        "\n"
      ],
      "metadata": {
        "id": "1xl4AeMl3jmb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pushing to the hub after training is as easy as:"
      ],
      "metadata": {
        "id": "7X7GV-YE5loA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
        "\n",
        "# hub_repo_name = REFEXP_MODEL_CHECKPOINT\n",
        "# hub_backup_repo_name = \"ivelin/donut-refexp-combined-v1-backup\"\n",
        "# local_save_dir=\"saved_pretrained\"\n",
        "# local_backup_save_dir=\"saved_pretrained_backup\"\n",
        "\n",
        "# def save_pretrained():\n",
        "#   global model_module, processor, model\n",
        "\n",
        "#   # save a copy to local disk storage just in case push to Hub is rejected.\n",
        "#   model_module.processor.save_pretrained(local_save_dir) # repo_id=\"ivelin/donut-refexp-combined-v1.1\") #, use_auth_token=\"...\") \n",
        "#   model_module.model.save_pretrained(local_save_dir) # backup_repo_name)\n",
        "\n",
        "#   # make sure we can load the model back from disk\n",
        "#   processor = processor.from_pretrained(local_save_dir)\n",
        "#   model = model.from_pretrained(local_save_dir)\n",
        "\n",
        "#   # save a backup\n",
        "#   model_module.processor.save_pretrained(local_backup_save_dir) # repo_id=\"ivelin/donut-refexp-combined-v1.1\") #, use_auth_token=\"...\") \n",
        "#   model_module.model.save_pretrained(local_backup_save_dir) # backup_repo_name)\n",
        "\n",
        "#   #\n",
        "#   # here we push the processor and model to the hub\n",
        "#   # note that you can add `private=True` in case you're using the private hub\n",
        "#   # which makes sure the model is only shared with your colleagues\n",
        "#   try:\n",
        "#     model_module.processor.push_to_hub(repo_id=hub_repo_name) \n",
        "#     model_module.model.push_to_hub(repo_id=hub_repo_name)\n",
        "\n",
        "#     processor = DonutProcessor.from_pretrained(hub_repo_name)\n",
        "#     model = VisionEncoderDecoderModel.from_pretrained(hub_repo_name)\n",
        "\n",
        "#     # save a backup in case uploading to the main model fails and corrupts the data\n",
        "#     model_module.processor.push_to_hub(repo_id=hub_backup_repo_name) \n",
        "#     model_module.model.push_to_hub(repo_id=hub_backup_repo_name)\n",
        "#   except Exception as e:\n",
        "#     print(\"Error pushing model to hub\", e)\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "gY24Xk8IDtNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save_pretrained()"
      ],
      "metadata": {
        "id": "chaFQM0R3mrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rinse and Repeat\n",
        "\n",
        "Now that we have confidence in the end to end training process, we can setup a continuous loop that runs unattended overnight.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vsEe8Dt8Fw1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #@ Set training on cruise control. Repeat the above cycle starting each pass with a fresh data subset.\n",
        "\n",
        "\n",
        "# # learning_rate= 1e-7 # latest recommended by trainer.tune()\n",
        "# print(f\"Learning rate set to {learning_rate}\")\n",
        "# # There is an issue with VisualEncoderDecoder when batch > 1 during training loss calculation\n",
        "# # train_batch_size = 1 \n",
        "# print(f\"Train batch size set to {train_batch_size}\")\n",
        "\n",
        "\n",
        "# def prep_next_data_subset():\n",
        "#   train_dataset = DonutDataset(REFEXP_DATASET_NAME, max_length=max_length, \n",
        "#                               range_samples=range_train_samples,\n",
        "#                               shuffle=True,\n",
        "#                               split=\"train\", task_start_token=\"<s_refexp>\", prompt_end_token=\"<s_target_bounding_box>\",\n",
        "#                               sort_json_key=False,\n",
        "#                               )\n",
        "#   val_dataset = DonutDataset(REFEXP_DATASET_NAME, max_length=max_length, range_samples=range_val_samples,\n",
        "#                               split=\"validation\", task_start_token=\"<s_refexp>\", prompt_end_token=\"<s_target_bounding_box>\",\n",
        "#                               sort_json_key=False,\n",
        "#                               )\n",
        "#   global train_dataloader, val_dataloader, train_batch_size\n",
        "#   train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "#   val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)  \n",
        "#   return train_dataloader, val_dataloader\n",
        "\n",
        "\n",
        "# def train_on_data_subset(train_dataloader=None, val_dataloader=None):\n",
        "#   global trainer, model_module\n",
        "#   trainer.fit(model_module, train_dataloader, val_dataloader) #, ckpt_path=\"last\")\n",
        "#   save_pretrained()\n",
        "\n",
        "# def tune_hparams():\n",
        "#   \"\"\"Use PL Tune the model to discover optimal hyper-parameters. \n",
        "#   More details here: https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html?highlight=batch_size#pytorch_lightning.trainer.Trainer.params.auto_scale_batch_size\"\"\"\n",
        "#   global trainer, model_module\n",
        "#   print(f\"Tuning trainer hyper parameters...\")\n",
        "#   trainer.tune(model_module)\n",
        "#   print(f\"Recommended batch_size: {model_module.batch_size}\")\n",
        "#   print(f\"Recommended learning rate: {model_module.learning_rate}\")\n",
        "#   return model_module.batch_size, model_module.learning_rate\n",
        "\n",
        "\n",
        "# def overnight_training():\n",
        "#   global train_dataset, val_dataset, max_epochs_per_loop\n",
        "#   total_train_samples = dataset['train'].num_rows\n",
        "#   total_epochs_per_loop = max_epochs_per_loop * int(total_train_samples/max_train_samples)\n",
        "#   # aiming for 100 passes of epoch loops over the whole dataset\n",
        "#   total_epochs = total_epochs_per_loop * 100\n",
        "#   for epoch_count in range(0, total_epochs, max_epochs_per_loop):\n",
        "#     epoch_loop_params = {\n",
        "#         'global_epoch_count': epoch_count,\n",
        "#         'train_batch_size': train_batch_size, \n",
        "#         'learning_rate': learning_rate\n",
        "#         }\n",
        "#     print(f'Starting epoch loop: {epoch_loop_params}.')\n",
        "#     wandb.log(epoch_loop_params)\n",
        "#     train_dataloader, val_dataloader = prep_next_data_subset()\n",
        "#     print(f'Data loaders prepared.')\n",
        "#     prep_trainer()\n",
        "#     print(f'Trainer prepared.')\n",
        "#     # tune hyper parameters every once in a while\n",
        "#     # docs: https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html?highlight=batch_size#pytorch_lightning.trainer.Trainer.params.auto_scale_batch_size\n",
        "#     # if epoch_count % total_epochs_per_loop == 0:\n",
        "#     #   optimal_batch_size, optimal_lr = tune_hparams()\n",
        "#     #   global train_batch_size, learning_rate\n",
        "#     #   # save to global space vars for interactive debugging\n",
        "#     #   # uncomment below after solving the train loss calc error\n",
        "#     #   # train_batch_size = optimal_batch_size\n",
        "#     #   learning_rate = optimal_lr\n",
        "#     #   # reconfigure trainer with optimal values\n",
        "#     #   config[\"lr\"] = optimal_lr\n",
        "#     #   config[\"train_batch_sizes\"] = [optimal_batch_size]\n",
        "#     #   print(f\"Train learning rate set to {optimal_lr}\")\n",
        "#     #   print(f\"Train batch size set to {optimal_batch_size}\")\n",
        "#     print(f'Starting training epoch loop.')\n",
        "#     train_on_data_subset(train_dataloader=train_dataloader, val_dataloader=val_dataloader)\n",
        "#     print(f'Ended training epoch loop.')\n"
      ],
      "metadata": {
        "id": "0VuSxzW_F1Rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# overnight_training()\n",
        "# \n"
      ],
      "metadata": {
        "id": "3wlOVLAMGK32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Bu-ixvEsRAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "IhVQ-stkd0Ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference\n",
        "\n",
        "For inference, you can use this [Gradio playground notebook](https://github.com/ivelin/donut_ui_refexp/blob/main/Inference_Playground_Donut_UI_RefExp_Gradio.ipynb) or this [Huggingface playspace](https://huggingface.co/spaces/ivelin/ui-refexp). Also see the Donut [docs](https://huggingface.co/docs/transformers/main/en/model_doc/donut#inference)."
      ],
      "metadata": {
        "id": "9t50qDh-lGMg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questions and Contributions\n",
        "\n",
        "For questions and suggestions, please open an Issue in the [github repo](https://github.com/ivelin/donut_ui_refexp). PRs are also most welcome."
      ],
      "metadata": {
        "id": "AJczyEB6Vq4G"
      }
    }
  ]
}
